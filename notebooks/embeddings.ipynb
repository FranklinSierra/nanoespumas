{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>**Loading trained networks**</font>\n",
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from numpy import asarray\n",
    "import tensorflow as tf\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "import imageio\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/tensorflow/examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_examples.models.pix2pix import pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 1\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "OUTPUT_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and reuse the Pix2Pix models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "generator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "\n",
    "discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n",
    "discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing optimizers, generatos and discriminators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"../models/translation/rgb/\" #----->folder where the model will be stored \n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator_g=generator_g,\n",
    "                           generator_f=generator_f,\n",
    "                           discriminator_x=discriminator_x,\n",
    "                           discriminator_y=discriminator_y,\n",
    "                           generator_g_optimizer=generator_g_optimizer,\n",
    "                           generator_f_optimizer=generator_f_optimizer,\n",
    "                           discriminator_x_optimizer=discriminator_x_optimizer,\n",
    "                           discriminator_y_optimizer=discriminator_y_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')\n",
    "\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(ckpt_manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making net test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in generator_g.layers:\n",
    "#     print(\"layer.name: \", layer.name)\n",
    "#     try:\n",
    "#         print(\"output shape: \", layer.output_shape)\n",
    "#     except:\n",
    "#         print(\"no output shape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out1 = generator_g.get_layer('sequential_8')\n",
    "# out2 = generator_g.get_layer('concatenate')\n",
    "# print(out1.output_shape)\n",
    "# print(out2.get_input_shape_at(0))\n",
    "# print(out2.get_input_shape_at(-1))\n",
    "# out2.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming 'out2' is a Concatenate layer\n",
    "# input_layer_name1 = out2.get_input_at(0)[0].name.split('/')[0]\n",
    "# input_layer_name2 = out2.get_input_at(0)[1].name.split('/')[0]\n",
    "\n",
    "# print(\"Layer name for get_input_shape_at(0):\", input_layer_name1)\n",
    "# print(\"Layer name for get_input_shape_at(1):\", input_layer_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #l1 = generator_g.get_layer(name='sequential_8')\n",
    "# #custom embedding space dim\n",
    "# initializer = tf.keras.initializers.Constant(1.)\n",
    "# l1 = generator_g.get_layer(name='concatenate')\n",
    "# input = l1.get_input_at(0)[0]\n",
    "# x = layers.Conv2D(filters=input.shape[-1], kernel_size=(2,2), kernel_initializer=initializer)(input)\n",
    "# x = layers.Reshape((input.shape[-1],))(x)\n",
    "# emb2 = Model(inputs=generator_g.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emb2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# until here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>**Load and preprocess data**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str2idx = {\n",
    "    'CuNi1': 0,\n",
    "    'CuNi2': 1,\n",
    "    'CuNi3': 2\n",
    "}\n",
    "\n",
    "idx2str = {\n",
    "    0: 'CuNi1',\n",
    "    1: 'CuNi2', \n",
    "    2: 'CuNi3'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_class(index):\n",
    "    \"\"\"\n",
    "    One-Hot Encoding for Classification Task\n",
    "\n",
    "    This function takes an integer 'index' representing the class label and performs\n",
    "    one-hot encoding for a classification task. One-hot encoding is a technique used\n",
    "    to convert categorical data (class labels) into a binary vector representation,\n",
    "    where the index of the class label is marked with 1 and all other elements are 0.\n",
    "\n",
    "    Parameters:\n",
    "        index (int): An integer representing the class label that needs to be one-hot encoded.\n",
    "                     It must be a non-negative integer less than the number of classes.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A NumPy array representing the one-hot encoded label. The length of\n",
    "                       the array is equal to the number of classes, and the element at the\n",
    "                       specified 'index' is set to 1, while all other elements are set to 0.\n",
    "\n",
    "    Example:\n",
    "        Suppose there are three classes: 0, 1, and 2. To one-hot encode class 1, use:\n",
    "        >>> class_index = 1\n",
    "        >>> encoded_label = ohe_class(class_index)\n",
    "        >>> print(encoded_label)\n",
    "        Output: [0 1 0]\n",
    "\n",
    "    Note:\n",
    "        The function assumes that the number of classes is fixed to 3, as the length of\n",
    "        the one-hot encoded label is hard-coded to 3. If your classification task involves\n",
    "        a different number of classes, you will need to modify the function accordingly.\n",
    "    \"\"\"\n",
    "    # Create an array of zeros with length 3 and integer data type\n",
    "    ohe_label = np.zeros(3, dtype=int)\n",
    "    \n",
    "    # Set the element at 'index' to 1 to represent the one-hot encoding\n",
    "    ohe_label[index] = 1\n",
    "    \n",
    "    # Return the one-hot encoded label as an array\n",
    "    return ohe_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the images to [-1, 1]\n",
    "def normalize(image):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image / 127.5) - 1\n",
    "    return image\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = normalize(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(path, size, pixels):\n",
    "    img2 = np.zeros((pixels.shape))\n",
    "    a = load_img(path, target_size=size, color_mode= \"grayscale\")\n",
    "    img2[:,:,0] = a\n",
    "    img2[:,:,1] = a\n",
    "    img2[:,:,2] = a\n",
    "\n",
    "    return img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(path, rgb, size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Load Images and Corresponding Labels from a Directory into Memory.\n",
    "\n",
    "    This function loads all images from a specified directory and their corresponding\n",
    "    labels (assumed to be encoded in the image filenames) into memory. The images are\n",
    "    loaded, resized to the specified dimensions, and converted into numpy arrays.\n",
    "    Labels are extracted from the filenames and converted into one-hot encoded vectors.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): The path to the directory containing the images.\n",
    "        size (tuple, optional): A tuple (width, height) specifying the target size\n",
    "                                for resizing the images. Default is (256, 256).\n",
    "        rgb (bool, optional): Set to True to load images in RGB color mode,\n",
    "                              False to load in grayscale mode. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A NumPy array containing the image data. Each element in the\n",
    "                       array is an image represented as a numpy array.\n",
    "        list: A list of one-hot encoded labels corresponding to each image in the\n",
    "              same order as the image data. Each label is represented as a NumPy\n",
    "              array of length equal to the number of classes.\n",
    "\n",
    "    Note:\n",
    "        The function uses Keras' 'load_img' and 'img_to_array' functions to load\n",
    "        and convert the images. Ensure that Keras or an appropriate library is\n",
    "        installed before using this function.\n",
    "\n",
    "    Example:\n",
    "        >>> data_path = \"/path/to/images/\"\n",
    "        >>> image_data, labels = load_images(data_path, size=(128, 128), rgb=True)\n",
    "        >>> print(image_data.shape)\n",
    "        Output: (num_images, 128, 128, 3)  # Assuming num_images is the total number of images.\n",
    "        >>> print(len(labels))\n",
    "        Output: num_images  # Number of images, each with a corresponding one-hot encoded label.\n",
    "    \"\"\"\n",
    "    data_list = list()\n",
    "    label_list = list()\n",
    "\n",
    "    # if not rgb:\n",
    "    #     color_mode = \"grayscale\"\n",
    "    # else:\n",
    "    #     color_mode = \"rgb\"\n",
    "\n",
    "    # Enumerate filenames in the directory, assuming all are images\n",
    "    for filename in tqdm(os.listdir(path)):\n",
    "        # Load and resize the image\n",
    "        pixels = load_img(os.path.join(path, filename), target_size=size, color_mode=\"rgb\")\n",
    "        # Convert to numpy array\n",
    "        pixels = img_to_array(pixels)\n",
    "\n",
    "        if rgb==False:\n",
    "            print(\"CONVIRTIENDO A GRAY SCALE!\")\n",
    "            #convert rgb to gray\n",
    "            pixels = rgb2gray(os.path.join(path, filename), size, pixels)\n",
    "        else:\n",
    "            None\n",
    "\n",
    "        # Store the image data\n",
    "        data_list.append(pixels)\n",
    "\n",
    "        # For labels\n",
    "        clase = filename.split('_')[0]\n",
    "        # Assuming 'str2idx' is a dictionary mapping class names to their respective indices\n",
    "        indx = str2idx[clase]\n",
    "        # Get one-hot encoding from the index\n",
    "        ohe_label = ohe_class(indx)\n",
    "        label_list.append(ohe_label)\n",
    "\n",
    "    return np.asarray(data_list), label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_emb(split, clase, group, embeddings, labels, dim):\n",
    "    \"\"\"\n",
    "    Save Embeddings, Labels, and Videos to Files.\n",
    "\n",
    "    This function takes embeddings, labels, and videos obtained from a model and\n",
    "    saves them to separate files for later use. The data is saved as NumPy arrays.\n",
    "\n",
    "    Parameters:\n",
    "        split (str): Indicates the data split, either 'train' or 'test', to determine\n",
    "                     the destination directory for saving the files.\n",
    "        clase (str): The class name or identifier to be included in the file names\n",
    "                     for better organization.\n",
    "        embeddings (list): A list of embeddings (feature vectors) obtained from a model.\n",
    "        labels (list): A list of one-hot encoded labels corresponding to the embeddings.\n",
    "        videos (list): A list of video data associated with the embeddings (optional).\n",
    "\n",
    "    Note:\n",
    "        The function converts the input lists 'embeddings', 'labels', and 'videos'\n",
    "        into NumPy arrays before saving them. Ensure that the data is properly formatted\n",
    "        before calling this function.\n",
    "    \"\"\"\n",
    "    # Convert the input lists to NumPy arrays\n",
    "    embeddings_arr = np.array(embeddings)\n",
    "    labels_arr = np.array(labels)\n",
    "\n",
    "    print(\"emb dimension: \", embeddings_arr.shape)\n",
    "    print(\"label dimension: \", labels_arr.shape)\n",
    "\n",
    "    # Create the file path based on the split and class name\n",
    "    if split == 'train':        \n",
    "        file_name = \"../embeddings/rgb/train_\" + group + \"/\" + clase + '/'\n",
    "    else:\n",
    "        file_name = \"../embeddings/rgb/test_\" + group + \"/\" + clase + '/'\n",
    "\n",
    "    print(\"saving on: \", file_name)\n",
    "\n",
    "    # Create the directory if it does not exist\n",
    "    if not os.path.exists(file_name):\n",
    "        os.makedirs(file_name)\n",
    "\n",
    "    # Save the embeddings, labels, and videos (if provided) as separate files\n",
    "    np.save(file_name + \"Embeddings\", embeddings_arr)\n",
    "    np.save(file_name + \"Labels\", labels_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toDataSet(path_origen, rgb):\n",
    "    \"\"\"\n",
    "    Convert Images and Labels to TensorFlow Dataset.\n",
    "\n",
    "    This function loads images and corresponding labels from a specified directory,\n",
    "    converts them into TensorFlow datasets, applies preprocessing to the images,\n",
    "    and returns a combined dataset containing the image and label pairs.\n",
    "\n",
    "    Parameters:\n",
    "        path_origen (str): The path to the directory containing the images.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: A TensorFlow dataset containing image and label pairs.\n",
    "                        The images are preprocessed and batched, and the labels\n",
    "                        are cast to int64 data type.\n",
    "\n",
    "    Note:\n",
    "        This function assumes that the 'load_images' function is defined and returns\n",
    "        a list of image data and labels. It also assumes the availability of 'BATCH_SIZE',\n",
    "        'AUTOTUNE', and 'BUFFER_SIZE' variables for data preprocessing.\n",
    "\n",
    "    \"\"\"\n",
    "    # Load images and labels using the 'load_images' function\n",
    "    data, labels = load_images(path_origen, rgb)\n",
    "\n",
    "    # Convert the data to NumPy array\n",
    "    data_array = np.asarray(data)\n",
    "\n",
    "    # Create a TensorFlow dataset for the image data\n",
    "    data_ds = tf.data.Dataset.from_tensor_slices(data_array)\n",
    "\n",
    "    # Create a TensorFlow dataset for the labels and batch them\n",
    "    labels_ds = tf.data.Dataset.from_tensor_slices(tf.cast(labels, tf.int64)).batch(BATCH_SIZE)\n",
    "\n",
    "    # Apply image preprocessing, cache, shuffle, and batch the image dataset\n",
    "    data_ds = data_ds.map(preprocess_image, num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "    # Combine the image and label datasets\n",
    "    data_label_ds = tf.data.Dataset.zip((data_ds, labels_ds))\n",
    "\n",
    "    return data_label_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>**Generator embeddings**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emb_model(generator, dim):\n",
    "    \"\"\"\n",
    "    Load Embedding Model from a Generator Model.\n",
    "\n",
    "    This function takes a generator model, extracts the intermediate embedding layer,\n",
    "    and creates a new model (embedding model) that outputs the embeddings obtained\n",
    "    from the intermediate layer. The function returns this new embedding model.\n",
    "\n",
    "    Parameters:\n",
    "        generator_g (tf.keras.Model): The generator model from which to extract\n",
    "                                      the intermediate embedding layer.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: A new TensorFlow model (embedding model) that takes the same\n",
    "                        input as the generator model and outputs the embeddings.\n",
    "\n",
    "    Note:\n",
    "        This function assumes that the generator model has a layer named 'concatenate'\n",
    "        representing the intermediate embedding layer.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"making emb model\")\n",
    "\n",
    "    #new\n",
    "    # initializer = tf.keras.initializers.Constant(1.)\n",
    "    # l1 = generator_g.get_layer(name='concatenate')\n",
    "    # input = l1.get_input_at(0)[0]\n",
    "    # x = layers.Conv2D(filters=input.shape[-1], kernel_size=(2,2), kernel_initializer=initializer)(input)\n",
    "    # x = layers.Reshape((input.shape[-1],))(x)\n",
    "    # emb2 = Model(inputs=generator_g.input, outputs=x)\n",
    "    #until here\n",
    "\n",
    "    #custom embedding space dim\n",
    "    initializer = tf.keras.initializers.Constant(1.)\n",
    "\n",
    "\n",
    "    if generator == \"generator_g\":\n",
    "        print(\"generator_g\")\n",
    "        generator = generator_g\n",
    "        l1 = generator.get_layer(name='concatenate')\n",
    "        input = l1.get_input_at(0)[0]\n",
    "        x = layers.Conv2D(filters=input.shape[-1], kernel_size=(2,2), kernel_initializer=initializer)(input)\n",
    "        x = layers.Reshape((input.shape[-1],))(x)\n",
    "        emb2 = Model(inputs=generator.input, outputs=x)\n",
    "    else:\n",
    "        print(\"generator_f\")\n",
    "        generator = generator_f\n",
    "        l1 = generator.get_layer(name='concatenate_1')\n",
    "        input = l1.get_input_at(0)[0]\n",
    "        x = layers.Conv2D(filters=input.shape[-1], kernel_size=(2,2), kernel_initializer=initializer)(input)\n",
    "        x = layers.Reshape((input.shape[-1],))(x)\n",
    "        emb2 = Model(inputs=generator.input, outputs=x)\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # emb = Model(generator.input, l1.output)\n",
    "\n",
    "    # #for solve problem\n",
    "    # #inputs = keras.Input(shape=(295, 2, 2, 1024), name='img')\n",
    "    # input = emb.output\n",
    "    # x = layers.Conv2D(filters=dim, kernel_size=(2,2), kernel_initializer=initializer)(input)\n",
    "    # x = layers.Reshape((dim,))(x)\n",
    "    # emb2 = Model(inputs=emb.input, outputs=x)\n",
    "    \n",
    "    return emb2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in generator_g.layers:\n",
    "#     print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #============= parcialmente resuelto ================\n",
    "# l1 = generator_g.get_layer(name='sequential_8')\n",
    "# # Get the last layer of the Sequential model\n",
    "# last_layer = l1.layers[-1]\n",
    "\n",
    "# # Calculate the size for the reshape layer\n",
    "# reshape_size = last_layer.output_shape[-1]  # Adjust this value as needed\n",
    "\n",
    "# # Create a Reshape layer with the desired size\n",
    "# reshape_layer = tf.keras.layers.Reshape(target_shape=(reshape_size,))\n",
    "\n",
    "# # Create a new model by appending the reshape layer to the end of the original model\n",
    "# reshaped_output = reshape_layer(last_layer.output)\n",
    "# decoder_model = tf.keras.Model(inputs=l1.input, outputs=reshaped_output)\n",
    "# decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if group == 'dry': generator = 'generator_g' else: generator = 'generator_f'\n",
    "split = 'train'\n",
    "group = 'wet'\n",
    "rgb = True\n",
    "dim = 512 #only for ablation study\n",
    "gen_path = '../imgs_results/rgb/' + split + '_' + group + '/'\n",
    "\n",
    "if group == \"dry\":\n",
    "     generator = 'generator_g'\n",
    "else:\n",
    "     generator = 'generator_f'\n",
    "\n",
    "\n",
    "\n",
    "#loading embedding model\n",
    "emb2 = load_emb_model(generator, dim)\n",
    "clases = ['CuNi1', 'CuNi2', 'CuNi3']\n",
    "\n",
    "for tipo in clases:\n",
    "    print(\"working on: \", tipo)\n",
    "    embeddings, labels = [], []\n",
    "    tipo_pth = gen_path + tipo + '/'\n",
    "    print(\"tipo_pth: \", tipo_pth)\n",
    "    clase = tipo\n",
    "    print(\"clase: \", clase)\n",
    "    print(\"convirtiendo a tf.Dataset...\")\n",
    "    data_ds = toDataSet(tipo_pth, rgb)\n",
    "    can = len(data_ds)\n",
    "    print(\"cantidad: \", can)\n",
    "    label = [clase]*can\n",
    "    labels.extend(label)\n",
    "    for img, label in tqdm(data_ds):\n",
    "        out = emb2(img)#emb2([img])#\n",
    "        embeddings.extend(out)\n",
    "    print(\"saving \", tipo, \" class...\")\n",
    "    saving_emb(split, clase, group, embeddings, labels, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np_path = \"../embeddings/rgb/test_dry/CuNi3/Embeddings.npy\"\n",
    "labels = np.load(np_path)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
