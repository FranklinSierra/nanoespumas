{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>**Loading trained networks**</font>\n",
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-05 21:33:46.629309: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imageio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2D\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imageio'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from numpy import asarray\n",
    "import tensorflow as tf\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "import imageio\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/tensorflow/examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_examples.models.pix2pix import pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 1\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "OUTPUT_CHANNELS = 3\n",
    "experiment = 'yourParticularModel'\n",
    "split = 'train'#changge if you want to get the train or test embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and reuse the Pix2Pix models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "generator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "\n",
    "discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n",
    "discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing optimizers, generatos and discriminators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"../models/folders/\" + experiment#----->folder where the model will be stored\n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator_g=generator_g,\n",
    "                           generator_f=generator_f,\n",
    "                           discriminator_x=discriminator_x,\n",
    "                           discriminator_y=discriminator_y,\n",
    "                           generator_g_optimizer=generator_g_optimizer,\n",
    "                           generator_f_optimizer=generator_f_optimizer,\n",
    "                           discriminator_x_optimizer=discriminator_x_optimizer,\n",
    "                           discriminator_y_optimizer=discriminator_y_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')\n",
    "\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(ckpt_manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>**Load and preprocess data**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str2idx = {\n",
    "    'adenoma': 0,\n",
    "    'hiperplastic': 1,\n",
    "    'serrated': 2\n",
    "}\n",
    "\n",
    "idx2str = {\n",
    "    0: 'adenoma',\n",
    "    1: 'hiperplastic', \n",
    "    2: 'serrated'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_class(index):\n",
    "    \"\"\"\n",
    "    One-Hot Encoding for Classification Task\n",
    "\n",
    "    This function takes an integer 'index' representing the class label and performs\n",
    "    one-hot encoding for a classification task. One-hot encoding is a technique used\n",
    "    to convert categorical data (class labels) into a binary vector representation,\n",
    "    where the index of the class label is marked with 1 and all other elements are 0.\n",
    "\n",
    "    Parameters:\n",
    "        index (int): An integer representing the class label that needs to be one-hot encoded.\n",
    "                     It must be a non-negative integer less than the number of classes.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A NumPy array representing the one-hot encoded label. The length of\n",
    "                       the array is equal to the number of classes, and the element at the\n",
    "                       specified 'index' is set to 1, while all other elements are set to 0.\n",
    "\n",
    "    Example:\n",
    "        Suppose there are three classes: 0, 1, and 2. To one-hot encode class 1, use:\n",
    "        >>> class_index = 1\n",
    "        >>> encoded_label = ohe_class(class_index)\n",
    "        >>> print(encoded_label)\n",
    "        Output: [0 1 0]\n",
    "\n",
    "    Note:\n",
    "        The function assumes that the number of classes is fixed to 3, as the length of\n",
    "        the one-hot encoded label is hard-coded to 3. If your classification task involves\n",
    "        a different number of classes, you will need to modify the function accordingly.\n",
    "    \"\"\"\n",
    "    # Create an array of zeros with length 3 and integer data type\n",
    "    ohe_label = np.zeros(3, dtype=int)\n",
    "    \n",
    "    # Set the element at 'index' to 1 to represent the one-hot encoding\n",
    "    ohe_label[index] = 1\n",
    "    \n",
    "    # Return the one-hot encoded label as an array\n",
    "    return ohe_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the images to [-1, 1]\n",
    "def normalize(image):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image / 127.5) - 1\n",
    "    return image\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = normalize(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(path, size=(256, 256), rgb=False):\n",
    "    \"\"\"\n",
    "    Load Images and Corresponding Labels from a Directory into Memory.\n",
    "\n",
    "    This function loads all images from a specified directory and their corresponding\n",
    "    labels (assumed to be encoded in the image filenames) into memory. The images are\n",
    "    loaded, resized to the specified dimensions, and converted into numpy arrays.\n",
    "    Labels are extracted from the filenames and converted into one-hot encoded vectors.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): The path to the directory containing the images.\n",
    "        size (tuple, optional): A tuple (width, height) specifying the target size\n",
    "                                for resizing the images. Default is (256, 256).\n",
    "        rgb (bool, optional): Set to True to load images in RGB color mode,\n",
    "                              False to load in grayscale mode. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A NumPy array containing the image data. Each element in the\n",
    "                       array is an image represented as a numpy array.\n",
    "        list: A list of one-hot encoded labels corresponding to each image in the\n",
    "              same order as the image data. Each label is represented as a NumPy\n",
    "              array of length equal to the number of classes.\n",
    "\n",
    "    Note:\n",
    "        The function uses Keras' 'load_img' and 'img_to_array' functions to load\n",
    "        and convert the images. Ensure that Keras or an appropriate library is\n",
    "        installed before using this function.\n",
    "\n",
    "    Example:\n",
    "        >>> data_path = \"/path/to/images/\"\n",
    "        >>> image_data, labels = load_images(data_path, size=(128, 128), rgb=True)\n",
    "        >>> print(image_data.shape)\n",
    "        Output: (num_images, 128, 128, 3)  # Assuming num_images is the total number of images.\n",
    "        >>> print(len(labels))\n",
    "        Output: num_images  # Number of images, each with a corresponding one-hot encoded label.\n",
    "    \"\"\"\n",
    "    data_list = list()\n",
    "    label_list = list()\n",
    "\n",
    "    if not rgb:\n",
    "        color_mode = \"grayscale\"\n",
    "    else:\n",
    "        color_mode = \"rgb\"\n",
    "\n",
    "    # Enumerate filenames in the directory, assuming all are images\n",
    "    for filename in tqdm(os.listdir(path)):\n",
    "        # Load and resize the image\n",
    "        pixels = load_img(os.path.join(path, filename), target_size=size, color_mode=color_mode)\n",
    "        # Convert to numpy array\n",
    "        pixels = img_to_array(pixels)\n",
    "        # Store the image data\n",
    "        data_list.append(pixels)\n",
    "\n",
    "        # For labels\n",
    "        clase = filename.split('_')[0]\n",
    "        # Assuming 'str2idx' is a dictionary mapping class names to their respective indices\n",
    "        indx = str2idx[clase]\n",
    "        # Get one-hot encoding from the index\n",
    "        ohe_label = ohe_class(indx)\n",
    "        label_list.append(ohe_label)\n",
    "\n",
    "    return np.asarray(data_list), label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_emb(split, clase, embeddings, labels, videos):\n",
    "    \"\"\"\n",
    "    Save Embeddings, Labels, and Videos to Files.\n",
    "\n",
    "    This function takes embeddings, labels, and videos obtained from a model and\n",
    "    saves them to separate files for later use. The data is saved as NumPy arrays.\n",
    "\n",
    "    Parameters:\n",
    "        split (str): Indicates the data split, either 'train' or 'test', to determine\n",
    "                     the destination directory for saving the files.\n",
    "        clase (str): The class name or identifier to be included in the file names\n",
    "                     for better organization.\n",
    "        embeddings (list): A list of embeddings (feature vectors) obtained from a model.\n",
    "        labels (list): A list of one-hot encoded labels corresponding to the embeddings.\n",
    "        videos (list): A list of video data associated with the embeddings (optional).\n",
    "\n",
    "    Note:\n",
    "        The function converts the input lists 'embeddings', 'labels', and 'videos'\n",
    "        into NumPy arrays before saving them. Ensure that the data is properly formatted\n",
    "        before calling this function.\n",
    "    \"\"\"\n",
    "    # Convert the input lists to NumPy arrays\n",
    "    embeddings_arr = np.array(embeddings)\n",
    "    labels_arr = np.array(labels)\n",
    "    videos_arr = np.array(videos)\n",
    "\n",
    "    print(\"emb dimension: \", embeddings_arr.shape)\n",
    "    print(\"label dimension: \", labels_arr.shape)\n",
    "    print(\"videos dimension: \", videos_arr.shape)\n",
    "\n",
    "    # Create the file path based on the split and class name\n",
    "    if split == 'train':        \n",
    "        file_name = \"/path/to_save/train/embeddings\" + clase\n",
    "    else:\n",
    "        file_name = \"/path/to_save/test/embeddings\" + clase\n",
    "\n",
    "    print(\"saving on: \", file_name)\n",
    "\n",
    "    # Create the directory if it does not exist\n",
    "    if not os.path.exists(file_name):\n",
    "        os.makedirs(file_name)\n",
    "\n",
    "    # Save the embeddings, labels, and videos (if provided) as separate files\n",
    "    np.save(file_name + \"Embeddings\", embeddings_arr)\n",
    "    np.save(file_name + \"Labels\", labels_arr)\n",
    "    np.save(file_name + \"Videos\", videos_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toDataSet(path_origen):\n",
    "    \"\"\"\n",
    "    Convert Images and Labels to TensorFlow Dataset.\n",
    "\n",
    "    This function loads images and corresponding labels from a specified directory,\n",
    "    converts them into TensorFlow datasets, applies preprocessing to the images,\n",
    "    and returns a combined dataset containing the image and label pairs.\n",
    "\n",
    "    Parameters:\n",
    "        path_origen (str): The path to the directory containing the images.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: A TensorFlow dataset containing image and label pairs.\n",
    "                        The images are preprocessed and batched, and the labels\n",
    "                        are cast to int64 data type.\n",
    "\n",
    "    Note:\n",
    "        This function assumes that the 'load_images' function is defined and returns\n",
    "        a list of image data and labels. It also assumes the availability of 'BATCH_SIZE',\n",
    "        'AUTOTUNE', and 'BUFFER_SIZE' variables for data preprocessing.\n",
    "\n",
    "    \"\"\"\n",
    "    # Load images and labels using the 'load_images' function\n",
    "    data, labels = load_images(path_origen, rgb=True)\n",
    "\n",
    "    # Convert the data to NumPy array\n",
    "    data_array = np.asarray(data)\n",
    "\n",
    "    # Create a TensorFlow dataset for the image data\n",
    "    data_ds = tf.data.Dataset.from_tensor_slices(data_array)\n",
    "\n",
    "    # Create a TensorFlow dataset for the labels and batch them\n",
    "    labels_ds = tf.data.Dataset.from_tensor_slices(tf.cast(labels, tf.int64)).batch(BATCH_SIZE)\n",
    "\n",
    "    # Apply image preprocessing, cache, shuffle, and batch the image dataset\n",
    "    data_ds = data_ds.map(preprocess_image, num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "    # Combine the image and label datasets\n",
    "    data_label_ds = tf.data.Dataset.zip((data_ds, labels_ds))\n",
    "\n",
    "    return data_label_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>**Generator embeddings**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emb_model(generator_g):\n",
    "    \"\"\"\n",
    "    Load Embedding Model from a Generator Model.\n",
    "\n",
    "    This function takes a generator model, extracts the intermediate embedding layer,\n",
    "    and creates a new model (embedding model) that outputs the embeddings obtained\n",
    "    from the intermediate layer. The function returns this new embedding model.\n",
    "\n",
    "    Parameters:\n",
    "        generator_g (tf.keras.Model): The generator model from which to extract\n",
    "                                      the intermediate embedding layer.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: A new TensorFlow model (embedding model) that takes the same\n",
    "                        input as the generator model and outputs the embeddings.\n",
    "\n",
    "    Note:\n",
    "        This function assumes that the generator model has a layer named 'concatenate'\n",
    "        representing the intermediate embedding layer.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"making emb model\")\n",
    "    l1 = generator_g.get_layer(name='concatenate')\n",
    "    emb = Model(generator_g.inputs, l1.output)\n",
    "\n",
    "    #for second concatenate layer\n",
    "    input = emb.output\n",
    "    b, w, h, d = input.shape \n",
    "    #x = tf.keras.layers.Conv2D(filters=dim, kernel_size=(2,2))(input)\n",
    "    x = tf.keras.layers.Reshape((4096,))(input)\n",
    "    emb2 = Model(inputs=emb.inputs, outputs=x)\n",
    "    \n",
    "    print(\"emb model done!\")\n",
    "    return emb2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_videos():\n",
    "    \n",
    "    path = \"../path/synthetic_imgs_results/test/\"\n",
    "    \n",
    "    folders = os.listdir(path)\n",
    "    check = 'WL'\n",
    "    folders = [folder for folder in folders if folder.split('_')[-1].upper() == check.upper()]\n",
    "    print(\"folders: \", folders)\n",
    "    videos_to_read = set()\n",
    "    for folder in folders:\n",
    "        print(folder)\n",
    "        folder_path = path + '/' + folder\n",
    "        videos = os.listdir(folder_path)\n",
    "        for video in videos:\n",
    "            name = folder + '/' + video \n",
    "            videos_to_read.add(name)\n",
    "\n",
    "    print(\"videos_to_read: \", len(videos_to_read))\n",
    "    videos_to_read = list(videos_to_read)\n",
    "    \n",
    "    check = 'a'\n",
    "    res_ade = [video for video in videos_to_read if video[0].lower() == check.lower()]\n",
    "    check = 'h'\n",
    "    res_hyp = [video for video in videos_to_read if video[0].lower() == check.lower()]\n",
    "    check = 's'\n",
    "    res_ser = [video for video in videos_to_read if video[0].lower() == check.lower()]\n",
    "\n",
    "    print(\"===== videos for label: =====\")\n",
    "    print(\"adenoma: \\n\", res_ade)\n",
    "    print(\"hyperplastic: \\n\", res_hyp)\n",
    "    print(\"serrated:  \\n\", res_ser)\n",
    "\n",
    "    return res_ade, res_hyp, res_ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_videos(): \n",
    "    path = \"../path/train_WL/\" \n",
    "    imgs = os.listdir(path)\n",
    "\n",
    "    videos_to_read = set()\n",
    "    for img in imgs:\n",
    "        info = img.split('.')[0]\n",
    "        folder = info.split('_')[0] + '_WL/'\n",
    "        video = info.split('_')[3]\n",
    "        name = folder + 'video_' + video\n",
    "        videos_to_read.add(name)\n",
    "\n",
    "\n",
    "    print(\"videos_to_read: \", len(videos_to_read))\n",
    "    videos_to_read = list(videos_to_read)\n",
    "\n",
    "    check = 'a'\n",
    "    res_ade = [video for video in videos_to_read if video[0].lower() == check.lower()]\n",
    "    check = 'h'\n",
    "    res_hyp = [video for video in videos_to_read if video[0].lower() == check.lower()]\n",
    "\n",
    "    print(\"===== videos for label:=====\")\n",
    "    print(\"adenoma: \\n\", res_ade)\n",
    "    print(\"hyperplastic: \\n\", res_hyp)\n",
    "    \n",
    "    videos_filt = res_ade + res_hyp\n",
    "    return imgs, sorted(videos_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_videos():\n",
    "    val_path = '../data/csv_files/adeVshyp/NBI/trainNBI.csv'\n",
    "    val_df = pd.read_csv(val_path, header=None)\n",
    "    val_df.columns = ['path', 'label']\n",
    "    val_df.groupby(['label']).count()\n",
    "    \n",
    "    videos = []\n",
    "    for i in range(len(val_df)):\n",
    "        path = val_df.iloc[i]['path']\n",
    "        info = path.split('/')[-1]\n",
    "        clase = info.split('_')[0]\n",
    "        video = info.split('_')[3]\n",
    "        to_save = clase + '_WL/video_' + video\n",
    "        videos.append(to_save)\n",
    "\n",
    "    videos_set = set(videos)\n",
    "    videos = list(videos_set)\n",
    "    \n",
    "    check = 'a'\n",
    "    res_ade = [video for video in videos if video[0].lower() == check.lower()]\n",
    "    check = 'h'\n",
    "    res_hyp = [video for video in videos if video[0].lower() == check.lower()]\n",
    "    check = 's'\n",
    "    res_ser = [video for video in videos if video[0].lower() == check.lower()]\n",
    "\n",
    "    print(\"===== videos for label: =====\")\n",
    "    print(\"adenoma: \\n\", res_ade)\n",
    "    print(\"hyperplastic: \\n\", res_hyp)\n",
    "    print(\"serrated:  \\n\", res_ser)\n",
    "\n",
    "    return res_ade, res_hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(get_val_videos())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating images \n",
    "<font color='red'>**For test set**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_path = '../../../../../data/polyp_original/WL/'\n",
    "split = 'test'\n",
    "\n",
    "#loading embedding model\n",
    "emb2 = load_emb_model(generator_g)\n",
    "\n",
    "res_ade, res_hyp, res_ser = get_test_videos()\n",
    "clases = [res_ade, res_hyp, res_ser]\n",
    "\n",
    "for tipo in clases:\n",
    "    print(\"working on: \", tipo)\n",
    "    embeddings, labels, all_videos = [], [], []#\n",
    "    for video in tipo:\n",
    "        video_pth = gen_path + video + '/'\n",
    "        print(\"video_pth: \", video_pth)\n",
    "        clase = video.split('/')[0].split('_')[0]\n",
    "        print(\"clase: \", clase)\n",
    "        video_num = video.split('/')[-1]\n",
    "        print(\"video_num: \", video_num)\n",
    "        print(\"convirtiendo a tf.Dataset...\")\n",
    "        data_ds = toDataSet(video_pth)\n",
    "        for img, label in tqdm(data_ds):\n",
    "            out = emb2(img)#emb2([img])#\n",
    "            #out = out[0][-1]\n",
    "            embeddings.extend(out)\n",
    "        can = len(data_ds)\n",
    "        print(\"cantidad: \", can)\n",
    "        label = [clase]*can\n",
    "        labels.extend(label)\n",
    "        curr_video = [video_num]*can\n",
    "        all_videos.extend(curr_video)\n",
    "    print(\"saving \", tipo, \" class...\")\n",
    "    saving_emb(split, clase, embeddings, labels, all_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**For train set**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_path = '../../../data/binary/'\n",
    "split = 'train'\n",
    "\n",
    "folds = ['fold1', 'fold2', 'fold3', 'fold4', 'fold5'] #--->for 5 kfold cross validation\n",
    "for fold in folds:\n",
    "    print(\"============== FOLD: \", fold, \" ===========\")\n",
    "    \n",
    "    #loading embedding model\n",
    "    emb2 = load_emb_model(generator_g)    \n",
    "    #getting all images and filtered videos over fold                \n",
    "    imgs, videos_filt = get_train_videos() \n",
    "    print(\"cantidad de videos a entrenar: \", len(videos_filt))\n",
    "    \n",
    "    for polyp_class in ['adenoma', 'hiperplastic']:\n",
    "        embeddings, all_labels, all_videos = [], [], []\n",
    "        print(\"working on: \", polyp_class)\n",
    "        related_videos = list(filter(lambda x: polyp_class in x, videos_filt)) \n",
    "        for single_video in related_videos: \n",
    "            data_list, label_list = list(), list()\n",
    "            clase = single_video.split('/')[0]\n",
    "            polyp_clase = clase.split('_')[0]\n",
    "            print(\"polyp_clase: \",polyp_clase)\n",
    "            video = single_video.split('/')[1]\n",
    "            video_num = video.split('_')[-1]\n",
    "            print(\"video_num: \", video_num)\n",
    "\n",
    "            check = polyp_clase + '_WL_video_' + video_num + '_'\n",
    "            print(\"working on: \", check)\n",
    "            related_imgs = list(filter(lambda x: check in x, imgs))\n",
    "            \n",
    "            for img in related_imgs:               \n",
    "                img_path = gen_path + fold + '/train_WL/' + img  \n",
    "                pixels = load_img(img_path, target_size=(256,256), color_mode= \"rgb\")\n",
    "                # convert to numpy array\n",
    "                pixels = img_to_array(pixels)\n",
    "                # store\n",
    "                data_list.append(pixels)\n",
    "\n",
    "                #for labels\n",
    "                clase = img.split('_')[0]\n",
    "                indx = str2idx[clase]\n",
    "                #get ohe from index\n",
    "                ohe_label = ohe_class(indx)\n",
    "                label_list.append(ohe_label)          \n",
    "\n",
    "            data, labels = asarray(data_list), label_list \n",
    "            data_array = np.asarray(data)\n",
    "            data_ds = tf.data.Dataset.from_tensor_slices(data_array)\n",
    "            labels_ds = tf.data.Dataset.from_tensor_slices(tf.cast(labels, tf.int64)).batch(BATCH_SIZE)\n",
    "            data_ds = data_ds.map(preprocess_image, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
    "                        BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "            data_ds = tf.data.Dataset.zip((data_ds, labels_ds))\n",
    "            for img, label in tqdm(data_ds):\n",
    "                out = emb2(img)#emb2([img])#\n",
    "                #out = out[0][-1]\n",
    "                embeddings.extend(out)\n",
    "            can = len(data_ds)\n",
    "            print(\"cantidad: \", can)\n",
    "            label = [polyp_clase]*can\n",
    "            all_labels.extend(label)\n",
    "            curr_video = [video_num]*can\n",
    "            all_videos.extend(curr_video)\n",
    "            \n",
    "        print(\"saving \", check, \" class...\")\n",
    "        saving_emb(split, polyp_clase, embeddings, all_labels, all_videos)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For serrated samples as test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_path = '../../../../../data/polyp_original/WL/serrated_WL/'\n",
    "embeddings, labels, all_videos = [], [], []\n",
    "\n",
    "videos = os.listdir(gen_path)\n",
    "for video in videos:\n",
    "    video_num = video.split('/')[-1]\n",
    "    print(video_num)\n",
    "    video_path = gen_path + video + '/'\n",
    "    data_ds = toDataSet(video_path)\n",
    "    for img, label in data_ds:\n",
    "        out = emb2(img)\n",
    "        embeddings.extend(out)\n",
    "    can = len(data_ds)\n",
    "    label = ['serrated']*can\n",
    "    labels.extend(label)\n",
    "    curr_video = [video_num]*can\n",
    "    all_videos.extend(curr_video)\n",
    "    \n",
    "print(\"saving ...\")\n",
    "saving_emb('serrated', embeddings, labels, all_videos, 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
